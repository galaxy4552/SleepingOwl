# ================================================
# ðŸ¦‰ NewCoolOwl - Homophone Extractor (TSI version)
# ================================================
# ç”¨é€”ï¼š
#   å¾ž tsi.csv è‡ªå‹•æ‰¾å‡ºåŒéŸ³ç•°è©ž
#   ä¸¦è¼¸å‡ºæˆ homophone_candidates.csv
#
# éœ€æ±‚ï¼š
#   pip install sentence-transformers tqdm
# ================================================

import csv
from collections import defaultdict
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import os

# -----------------------------------------------
# è·¯å¾‘è¨­å®š
# -----------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(BASE_DIR, "tsi.csv")
OUTPUT_PATH = os.path.join(BASE_DIR, "homophone_candidates.csv")

# -----------------------------------------------
# Step 1. è®€å…¥è©žåº«ä¸¦æŒ‰æ³¨éŸ³åˆ†çµ„
# -----------------------------------------------
if not os.path.exists(DATA_PATH):
    print(f"[éŒ¯èª¤] æ‰¾ä¸åˆ°è©žåº«æª”æ¡ˆï¼š{DATA_PATH}")
    print("è«‹ç¢ºèª tsi.csv å·²å­˜åœ¨ã€‚")
    exit(1)

groups = defaultdict(list)
with open(DATA_PATH, "r", encoding="utf-8") as f:
    reader = csv.reader(f)
    for row in reader:
        if len(row) != 3:
            continue
        word, freq, zhuyin = row
        try:
            freq = int(freq)
        except ValueError:
            continue
        groups[zhuyin].append((word, freq))

# åªä¿ç•™æœ‰å¤šæ–¼ 1 å€‹è©žçš„æ³¨éŸ³ç¾¤
groups = {k: v for k, v in groups.items() if len(v) > 1}
print(f"åˆ†çµ„å®Œæˆï¼Œå…± {len(groups)} çµ„å…·æœ‰å¤šå€‹åŒéŸ³è©žã€‚")

# -----------------------------------------------
# Step 2. è¼‰å…¥ embedding æ¨¡åž‹
# -----------------------------------------------
print("è¼‰å…¥æ¨¡åž‹ BAAI/bge-small-zh-v1.5 ...")
model = SentenceTransformer("BAAI/bge-small-zh-v1.5")

# -----------------------------------------------
# Step 3. è¨ˆç®—é »çŽ‡æ¯”èˆ‡èªžç¾©ç›¸ä¼¼åº¦
# -----------------------------------------------
results = []
for zy, words in tqdm(groups.items(), desc="è¨ˆç®—ä¸­"):
    # æŒ‰è©žé »ç”±é«˜åˆ°ä½ŽæŽ’åº
    words.sort(key=lambda x: x[1], reverse=True)
    main_word, main_freq = words[0]

    # è‹¥ä¸»è¦è©žé »ç‚º 0ï¼Œç•¥éŽè©²çµ„
    if main_freq == 0:
        continue

    main_vec = model.encode(main_word, normalize_embeddings=True)

    for word, freq in words[1:]:
        if main_freq == 0:  # å†æ¬¡ä¿éšª
            continue
        freq_ratio = round(freq / main_freq, 3)

        word_vec = model.encode(word, normalize_embeddings=True)
        sim = util.cos_sim(main_vec, word_vec).item()
        if sim > 0.7 and freq_ratio < 0.5:
            results.append([zy, word, main_word, freq_ratio, round(sim, 3)])

# -----------------------------------------------
# Step 4. è¼¸å‡ºçµæžœ
# -----------------------------------------------
with open(OUTPUT_PATH, "w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["zhuyin", "wrong_word", "correct_word", "freq_ratio", "semantic_sim"])
    writer.writerows(results)

print()
print(f"âœ… å®Œæˆï¼Œå…±è¼¸å‡º {len(results)} ç­†çµæžœï¼š{OUTPUT_PATH}")
